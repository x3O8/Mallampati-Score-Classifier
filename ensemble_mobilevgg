import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.applications import MobileNetV2, VGG16
from sklearn.model_selection import train_test_split, KFold
from sklearn.utils import class_weight
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
import seaborn as sns
import glob
import logging

# --- 1. SETUP ---
# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

print(f"TensorFlow Version: {tf.__version__}")

class Config:
    """Global configuration class for the Kaggle project."""
    # Data and Path settings
    # IMPORTANT: Replace '<your-dataset-directory>' with the name of your dataset folder in /kaggle/input/
    # For example, if your data is in /kaggle/input/mallampati-data/augmented_data, set to:
    # DATA_PATH = "/kaggle/input/mallampati-data/augmented_data"
    DATA_PATH = "/kaggle/input/mallampati-data2/augmented_data"
    MODEL_PATH = "/kaggle/working/models/"
    RESULTS_PATH = "/kaggle/working/results/"
    
    # Dataset and Class settings
    CLASSES = ["1", "2", "3", "4"]
    
    # Model and Training Hyperparameters
    IMG_SIZE = (224, 224)
    BATCH_SIZE = 8
    NUM_CLASSES = 4
    EPOCHS = 50
    RANDOM_SEED = 42
    
    # K-Fold Cross-Validation
    N_SPLITS = 3
    
    # Test-Time Augmentation
    TTA_STEPS = 5
    
    # Model-specific Hyperparameters (aligned with research paper)
    MODEL_CONFIGS = {
        'MobileNetV2': {
            'lr': 5e-5,
            'unfreeze_layers': -10,
            'dense_units': 128,
            'l2_lambda': 0.01,
            'dropout_rate': 0.5
        },
        'VGG16_v1': {
            'lr': 5e-5,
            'unfreeze_block': 'block5',
            'dense_units': 32,
            'l2_lambda': 0.01,
            'dropout_rate': 0.5
        },
        'VGG16_v2': {
            'lr': 1e-4,
            'unfreeze_layers': ['block5_conv2', 'block5_conv3'],
            'dense_units': 24,
            'l2_lambda': 0.03,
            'dropout_rate': 0.6
        }
    }

def setup_environment():
    """Set random seeds and create necessary directories for Kaggle."""
    os.environ['PYTHONHASHSEED'] = str(Config.RANDOM_SEED)
    tf.random.set_seed(Config.RANDOM_SEED)
    np.random.seed(Config.RANDOM_SEED)
    
    os.makedirs(Config.MODEL_PATH, exist_ok=True)
    os.makedirs(Config.RESULTS_PATH, exist_ok=True)
    logger.info("Kaggle environment setup complete. Output directories created.")

# ==============================================================================
# Part 2: High-Performance Data Ingestion and Augmentation Pipeline
# ==============================================================================
def load_data():
    """Loads image paths and labels from train, val, and test directories."""
    filepaths = []
    labels = []
    subdirs = ['train', 'val', 'test']
    
    # Check if DATA_PATH exists
    if not os.path.exists(Config.DATA_PATH):
        logger.error(f"DATA_PATH {Config.DATA_PATH} does not exist.")
        return np.array([]), np.array([])
    
    # Log directory contents
    logger.info(f"Checking directory: {Config.DATA_PATH}")
    found_subdirs = [d for d in os.listdir(Config.DATA_PATH) if os.path.isdir(os.path.join(Config.DATA_PATH, d))]
    logger.info(f"Subdirectories found: {found_subdirs}")
    
    for subdir in subdirs:
        subdir_path = os.path.join(Config.DATA_PATH, subdir)
        if not os.path.exists(subdir_path):
            logger.warning(f"Subdirectory {subdir_path} not found.")
            continue
        
        for i, class_name in enumerate(Config.CLASSES):
            class_dir = os.path.join(subdir_path, class_name)
            if not os.path.exists(class_dir):
                logger.warning(f"Class directory {class_dir} not found.")
                continue
            
            image_paths = glob.glob(os.path.join(class_dir, '*.jpg')) + \
                          glob.glob(os.path.join(class_dir, '*.png')) + \
                          glob.glob(os.path.join(class_dir, '*.JPG')) + \
                          glob.glob(os.path.join(class_dir, '*.PNG'))
            logger.info(f"Class {class_name} in {subdir}: Found {len(image_paths)} images in {class_dir}")
            
            if not image_paths:
                logger.warning(f"No valid images found in {class_dir}.")
                continue
            
            filepaths.extend(image_paths)
            labels.extend([i] * len(image_paths))
    
    filepaths = np.array(filepaths)
    labels = np.array(labels)
    
    if len(filepaths) == 0:
        logger.error("No images found across all classes and subdirectories. Please check the directory structure and file formats.")
    else:
        logger.info(f"Total images found: {len(filepaths)} across {len(np.unique(labels))} classes.")
    
    return filepaths, labels

def get_training_augmentation_pipeline():
    """Returns the data augmentation pipeline for training."""
    return keras.Sequential([
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.1),
        layers.RandomContrast(0.1),
    ], name='training_augmentation')

def get_tta_augmentation_pipeline():
    """Returns the data augmentation pipeline for Test-Time Augmentation."""
    return keras.Sequential([
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.05),
    ], name='tta_augmentation')

def process_path(file_path, label):
    """Loads, decodes, and preprocesses an image file."""
    img = tf.io.read_file(file_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, Config.IMG_SIZE)
    label = tf.one_hot(label, Config.NUM_CLASSES)
    return img, label

def create_dataset(filepaths, labels, is_training=True):
    """Creates a tf.data.Dataset for training or validation."""
    dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))
    
    if is_training:
        dataset = dataset.shuffle(buffer_size=len(filepaths), seed=Config.RANDOM_SEED)
    
    dataset = dataset.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.batch(Config.BATCH_SIZE)
    
    if is_training:
        aug_pipeline = get_training_augmentation_pipeline()
        dataset = dataset.map(lambda x, y: (aug_pipeline(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)
    
    # Apply rescaling to all datasets
    dataset = dataset.map(lambda x, y: (layers.Rescaling(1./255)(x), y), num_parallel_calls=tf.data.AUTOTUNE)
        
    return dataset.prefetch(buffer_size=tf.data.AUTOTUNE)

# ==============================================================================
# Part 3: Architectural Deep Dive: Constructing the Ensemble Models
# ==============================================================================
def build_model(model_name):
    """Builds and returns one of the three specified models."""
    cfg = Config.MODEL_CONFIGS[model_name]
    
    if model_name == 'MobileNetV2':
        base_model = MobileNetV2(input_shape=Config.IMG_SIZE + (3,), include_top=False, weights='imagenet')
        base_model.trainable = True
        for layer in base_model.layers[:cfg['unfreeze_layers']]:
            layer.trainable = False
    elif 'VGG16' in model_name:
        base_model = VGG16(input_shape=Config.IMG_SIZE + (3,), include_top=False, weights='imagenet')
        base_model.trainable = True
        if 'unfreeze_block' in cfg:  # VGG16_v1
            for layer in base_model.layers:
                if not layer.name.startswith(cfg['unfreeze_block']):
                    layer.trainable = False
        else:  # VGG16_v2
            for layer in base_model.layers:
                if layer.name not in cfg['unfreeze_layers']:
                    layer.trainable = False
    else:
        raise ValueError(f"Unknown model name: {model_name}")

    inputs = keras.Input(shape=Config.IMG_SIZE + (3,))
    x = base_model(inputs, training=False)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(cfg['dense_units'], activation='relu', kernel_regularizer=regularizers.l2(cfg['l2_lambda']))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(cfg['dropout_rate'])(x)
    outputs = layers.Dense(Config.NUM_CLASSES, activation='softmax')(x)
    
    model = models.Model(inputs, outputs, name=model_name)
    
    optimizer = keras.optimizers.Adam(learning_rate=cfg['lr'])
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model

# ==============================================================================
# Part 4: The Two-Phase Training Protocol
# ==============================================================================
def get_callbacks(model_name, fold=None):
    """Defines the Keras callbacks for training."""
    suffix = f"_fold_{fold}" if fold else "_final"
    model_checkpoint_path = os.path.join(Config.MODEL_PATH, f"{model_name}{suffix}.keras")
    
    return [
        keras.callbacks.ModelCheckpoint(
            model_checkpoint_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1
        ),
        keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7)
    ]

def train_pipeline(X, y):
    """Executes the full two-phase training protocol."""
    X_pool, X_test, y_pool, y_test = train_test_split(
        X, y, test_size=0.2, random_state=Config.RANDOM_SEED, stratify=y
    )
    logger.info(f"Data split: {len(X_pool)} for training pool, {len(X_test)} for testing.")

    # --- Phase 1: K-Fold Cross-Validation ---
    logger.info("\n--- Starting Phase 1: 3-Fold Cross-Validation ---")
    kfold = KFold(n_splits=Config.N_SPLITS, shuffle=True, random_state=Config.RANDOM_SEED)
    
    for model_name in Config.MODEL_CONFIGS.keys():
        logger.info(f"\nCross-validating model: {model_name}")
        fold_accuracies = []
        for fold, (train_idx, val_idx) in enumerate(kfold.split(X_pool, y_pool)):
            logger.info(f"  Fold {fold+1}/{Config.N_SPLITS}")
            X_train, X_val = X_pool[train_idx], X_pool[val_idx]
            y_train, y_val = y_pool[train_idx], y_pool[val_idx]
            
            train_ds = create_dataset(X_train, y_train, is_training=True)
            val_ds = create_dataset(X_val, y_val, is_training=False)
            
            class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
            class_weights_dict = dict(enumerate(class_weights))
            
            model = build_model(model_name)
            history = model.fit(
                train_ds,
                epochs=Config.EPOCHS,
                validation_data=val_ds,
                class_weight=class_weights_dict,
                callbacks=get_callbacks(model_name, fold=fold+1),
                verbose=2
            )
            val_acc = max(history.history['val_accuracy'])
            fold_accuracies.append(val_acc)
            logger.info(f"  Fold {fold+1} Validation Accuracy: {val_acc:.4f}")
        
        logger.info(f"  {model_name} Average CV Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})")

    # --- Phase 2: Final Model Training ---
    logger.info("\n--- Starting Phase 2: Final Model Training on Full Pool ---")
    for model_name in Config.MODEL_CONFIGS.keys():
        logger.info(f"\nTraining final model: {model_name}")
        
        X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(
            X_pool, y_pool, test_size=0.1, random_state=Config.RANDOM_SEED, stratify=y_pool
        )
        
        train_ds_final = create_dataset(X_train_final, y_train_final, is_training=True)
        val_ds_final = create_dataset(X_val_final, y_val_final, is_training=False)
        
        class_weights_final = class_weight.compute_class_weight('balanced', classes=np.unique(y_train_final), y=y_train_final)
        class_weights_dict_final = dict(enumerate(class_weights_final))
        
        model = build_model(model_name)
        model.fit(
            train_ds_final,
            epochs=Config.EPOCHS,
            validation_data=val_ds_final,
            class_weight=class_weights_dict_final,
            callbacks=get_callbacks(model_name),
            verbose=1
        )
        logger.info(f"Final model for {model_name} trained and saved.")
        
    return X_test, y_test

# ==============================================================================
# Part 5: Advanced Inference: Ensemble Fusion with TTA
# ==============================================================================
def ensemble_predict_with_tta(X_test, y_test):
    """Performs inference using the ensemble with TTA."""
    logger.info("\n--- Starting Inference with Ensemble and TTA ---")
    
    models_list = []
    for model_name in Config.MODEL_CONFIGS.keys():
        model_path = os.path.join(Config.MODEL_PATH, f"{model_name}_final.keras")
        if os.path.exists(model_path):
            logger.info(f"Loading model: {model_path}")
            models_list.append(keras.models.load_model(model_path))
        else:
            logger.error(f"Model file not found at {model_path}. Please run training first.")
            return None, None
            
    if not models_list:
        return None, None

    tta_augmenter = get_tta_augmentation_pipeline()
    rescaler = layers.Rescaling(1./255)
    all_preds = []
    
    test_ds = tf.data.Dataset.from_tensor_slices(X_test).map(
        lambda x: tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(x), channels=3), Config.IMG_SIZE)
    ).batch(1)

    for i, image_tensor in enumerate(test_ds):
        print(f"  Predicting on test image {i+1}/{len(X_test)}", end='\r')
        
        model_avg_preds = []
        for model in models_list:
            tta_preds = []
            for _ in range(Config.TTA_STEPS):
                augmented_image = tta_augmenter(image_tensor, training=True)
                rescaled_image = rescaler(augmented_image)
                pred = model.predict(rescaled_image, verbose=0)
                tta_preds.append(pred)
            
            model_avg_preds.append(np.mean(tta_preds, axis=0))
            
        final_pred_probs = np.mean(model_avg_preds, axis=0)
        all_preds.append(final_pred_probs.flatten())

    print("\nInference complete.                ")
    y_pred_classes = np.argmax(all_preds, axis=1)
    y_pred_probs = np.array(all_preds)
    
    return y_pred_classes, y_pred_probs

# ==============================================================================
# Part 6: Comprehensive Performance Evaluation and Results Analysis
# ==============================================================================
def evaluate_performance(y_true, y_pred_classes, y_pred_probs):
    """Generates and saves all evaluation metrics and plots."""
    logger.info("\n--- Generating Performance Evaluation Report ---")
    
    # 1. Classification Report
    report = classification_report(y_true, y_pred_classes, target_names=Config.CLASSES, digits=4)
    print("\nClassification Report:")
    print(report)
    with open(os.path.join(Config.RESULTS_PATH, "classification_report.txt"), "w") as f:
        f.write(report)
    
    # 2. Confusion Matrix
    cm = confusion_matrix(y_true, y_pred_classes)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=Config.CLASSES, yticklabels=Config.CLASSES)
    plt.title('Ensemble Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig(os.path.join(Config.RESULTS_PATH, "confusion_matrix.png"))
    plt.show()
    logger.info("Confusion matrix saved and displayed.")

    y_true_one_hot = tf.one_hot(y_true, Config.NUM_CLASSES).numpy()
    
    # 3. ROC Curves
    plt.figure(figsize=(10, 8))
    for i in range(Config.NUM_CLASSES):
        fpr, tpr, _ = roc_curve(y_true_one_hot[:, i], y_pred_probs[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=2, label=f'ROC curve of {Config.CLASSES[i]} (area = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Multi-class Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.savefig(os.path.join(Config.RESULTS_PATH, "roc_curves.png"))
    plt.show()
    logger.info("ROC curves saved and displayed.")
    
    # 4. Precision-Recall Curves
    plt.figure(figsize=(10, 8))
    for i in range(Config.NUM_CLASSES):
        precision, recall, _ = precision_recall_curve(y_true_one_hot[:, i], y_pred_probs[:, i])
        avg_precision = average_precision_score(y_true_one_hot[:, i], y_pred_probs[:, i])
        plt.plot(recall, precision, lw=2, label=f'PR curve of {Config.CLASSES[i]} (AP = {avg_precision:.4f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Multi-class Precision-Recall Curves')
    plt.legend(loc="best")
    plt.savefig(os.path.join(Config.RESULTS_PATH, "pr_curves.png"))
    plt.show()
    logger.info("Precision-Recall curves saved and displayed.")

# ==============================================================================
# Main Execution Block
# ==============================================================================
if __name__ == "__main__":
    setup_environment()
    
    # Step 1: Load data paths and labels
    X, y = load_data()
    
    if len(X) == 0:
        logger.error(f"FATAL: No data found in {Config.DATA_PATH}. Please check the path and directory structure.")
    else:
        logger.info(f"Successfully loaded {len(X)} images.")
        
        # Step 2: Run the two-phase training protocol
        X_test, y_test = train_pipeline(X, y)
        
        # Step 3: Run inference on the test set
        y_pred_classes, y_pred_probs = ensemble_predict_with_tta(X_test, y_test)
        
        # Step 4: Evaluate the final performance
        if y_pred_classes is not None:
            evaluate_performance(y_test, y_pred_classes, y_pred_probs)
            logger.info("\n✅ Pipeline finished successfully. Check the '/kaggle/working/results/' directory for output files.")